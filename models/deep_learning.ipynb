{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c460ba66",
   "metadata": {},
   "source": [
    "# Preprocess data for deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6702f449",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout, Concatenate\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import set_random_seed\n",
    "from tensorflow.keras.regularizers import L2\n",
    "from sklearn.utils import class_weight\n",
    "from keras.layers import Bidirectional\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f98d230a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(82486, 17)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>body</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "      <th>subject_len</th>\n",
       "      <th>body_len</th>\n",
       "      <th>subject_density</th>\n",
       "      <th>body_density</th>\n",
       "      <th>num_exclamations</th>\n",
       "      <th>body_entropy</th>\n",
       "      <th>body_entropy_per_char</th>\n",
       "      <th>percent_digits</th>\n",
       "      <th>percent_punct</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Re: New Sequences Window</td>\n",
       "      <td>Date:        Wed, 21 Aug 2002 10:54:46 -0500  ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Assassin</td>\n",
       "      <td>24</td>\n",
       "      <td>1538</td>\n",
       "      <td>4.800000</td>\n",
       "      <td>6.835556</td>\n",
       "      <td>0</td>\n",
       "      <td>4.9731</td>\n",
       "      <td>0.003233</td>\n",
       "      <td>0.0670</td>\n",
       "      <td>0.1268</td>\n",
       "      <td>Re: New Sequences Window Date:        Wed, 21 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[zzzzteana] RE: Alexander</td>\n",
       "      <td>Martin A posted:\\nTassos Papadopoulos, the Gre...</td>\n",
       "      <td>0</td>\n",
       "      <td>Assassin</td>\n",
       "      <td>25</td>\n",
       "      <td>894</td>\n",
       "      <td>6.250000</td>\n",
       "      <td>7.982143</td>\n",
       "      <td>2</td>\n",
       "      <td>4.6876</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>0.0134</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>[zzzzteana] RE: Alexander Martin A posted:\\nTa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[zzzzteana] Moscow bomber</td>\n",
       "      <td>Man Threatens Explosion In Moscow \\n\\nThursday...</td>\n",
       "      <td>0</td>\n",
       "      <td>Assassin</td>\n",
       "      <td>25</td>\n",
       "      <td>1746</td>\n",
       "      <td>6.250000</td>\n",
       "      <td>6.901186</td>\n",
       "      <td>2</td>\n",
       "      <td>4.7850</td>\n",
       "      <td>0.002741</td>\n",
       "      <td>0.0074</td>\n",
       "      <td>0.1042</td>\n",
       "      <td>[zzzzteana] Moscow bomber Man Threatens Explos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[IRR] Klez: The Virus That  Won't Die</td>\n",
       "      <td>Klez: The Virus That Won't Die\\n \\nAlready the...</td>\n",
       "      <td>0</td>\n",
       "      <td>Assassin</td>\n",
       "      <td>37</td>\n",
       "      <td>1125</td>\n",
       "      <td>4.625000</td>\n",
       "      <td>6.818182</td>\n",
       "      <td>0</td>\n",
       "      <td>4.7567</td>\n",
       "      <td>0.004228</td>\n",
       "      <td>0.0240</td>\n",
       "      <td>0.0818</td>\n",
       "      <td>[IRR] Klez: The Virus That  Won't Die Klez: Th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Re: [zzzzteana] Nothing like mama used to make</td>\n",
       "      <td>&gt;  in adding cream to spaghetti carbonara, whi...</td>\n",
       "      <td>0</td>\n",
       "      <td>Assassin</td>\n",
       "      <td>46</td>\n",
       "      <td>1047</td>\n",
       "      <td>5.111111</td>\n",
       "      <td>7.270833</td>\n",
       "      <td>2</td>\n",
       "      <td>4.7307</td>\n",
       "      <td>0.004518</td>\n",
       "      <td>0.0038</td>\n",
       "      <td>0.1691</td>\n",
       "      <td>Re: [zzzzteana] Nothing like mama used to make...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          subject  \\\n",
       "0                        Re: New Sequences Window   \n",
       "1                       [zzzzteana] RE: Alexander   \n",
       "2                       [zzzzteana] Moscow bomber   \n",
       "3           [IRR] Klez: The Virus That  Won't Die   \n",
       "4  Re: [zzzzteana] Nothing like mama used to make   \n",
       "\n",
       "                                                body  label    source  \\\n",
       "0  Date:        Wed, 21 Aug 2002 10:54:46 -0500  ...      0  Assassin   \n",
       "1  Martin A posted:\\nTassos Papadopoulos, the Gre...      0  Assassin   \n",
       "2  Man Threatens Explosion In Moscow \\n\\nThursday...      0  Assassin   \n",
       "3  Klez: The Virus That Won't Die\\n \\nAlready the...      0  Assassin   \n",
       "4  >  in adding cream to spaghetti carbonara, whi...      0  Assassin   \n",
       "\n",
       "   subject_len  body_len  subject_density  body_density  num_exclamations  \\\n",
       "0           24      1538         4.800000      6.835556                 0   \n",
       "1           25       894         6.250000      7.982143                 2   \n",
       "2           25      1746         6.250000      6.901186                 2   \n",
       "3           37      1125         4.625000      6.818182                 0   \n",
       "4           46      1047         5.111111      7.270833                 2   \n",
       "\n",
       "   body_entropy  body_entropy_per_char  percent_digits  percent_punct  \\\n",
       "0        4.9731               0.003233          0.0670         0.1268   \n",
       "1        4.6876               0.005243          0.0134         0.2069   \n",
       "2        4.7850               0.002741          0.0074         0.1042   \n",
       "3        4.7567               0.004228          0.0240         0.0818   \n",
       "4        4.7307               0.004518          0.0038         0.1691   \n",
       "\n",
       "                                                text  \n",
       "0  Re: New Sequences Window Date:        Wed, 21 ...  \n",
       "1  [zzzzteana] RE: Alexander Martin A posted:\\nTa...  \n",
       "2  [zzzzteana] Moscow bomber Man Threatens Explos...  \n",
       "3  [IRR] Klez: The Virus That  Won't Die Klez: Th...  \n",
       "4  Re: [zzzzteana] Nothing like mama used to make...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../dataset/3_features_phishing_enhanced.csv\")\n",
    "print(df.shape)\n",
    "df = df.drop(columns=['num_links', 'num_special_chars', 'has_bank_word'])\n",
    "df_filtered = df[df['source'] != 'Nazario'] # provo a togliere Nazario\n",
    "df = df_filtered\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35e2f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”„ KERAS LOSO: Validating on source: Assassin\n",
      "\u001b[1m182/182\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 48ms/step\n",
      "\n",
      "ğŸ”„ KERAS LOSO: Validating on source: CEAS-08\n",
      "\u001b[1m1224/1224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 47ms/step\n",
      "\n",
      "ğŸ”„ KERAS LOSO: Validating on source: Nigerian_Fraud\n",
      "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 48ms/step\n",
      "\n",
      "ğŸ”„ KERAS LOSO: Validating on source: Nazario\n",
      "\u001b[1m49/49\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 48ms/step\n",
      "\n",
      "ğŸ”„ KERAS LOSO: Validating on source: Enron\n",
      "\u001b[1m931/931\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 50ms/step\n",
      "\n",
      "ğŸ”„ KERAS LOSO: Validating on source: Ling\n",
      "\u001b[1m90/90\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 48ms/step\n",
      "\n",
      "ğŸ“Š KERAS LSTM LOSO Results:\n",
      "                 support  accuracy  precision  recall  f1-score\n",
      "Assassin         5809.0     0.786      0.587   0.933     0.721\n",
      "CEAS-08         39154.0     0.818      0.844   0.825     0.835\n",
      "Nigerian_Fraud   3332.0     0.954      1.000   0.954     0.977\n",
      "Nazario          1565.0     0.519      1.000   0.519     0.683\n",
      "Enron           29767.0     0.723      0.767   0.589     0.666\n",
      "Ling             2859.0     0.895      0.615   0.924     0.738\n"
     ]
    }
   ],
   "source": [
    "set_random_seed(42)     # per riproducibilitÃ \n",
    "\n",
    "# Settings\n",
    "MAX_NUM_WORDS = 10000\n",
    "MAX_SEQUENCE_LENGTH = 300\n",
    "text_col = 'text'\n",
    "num_cols = [\n",
    "    'subject_len', 'body_len', 'subject_density', 'body_density',\n",
    "    'num_exclamations', 'percent_punct',\n",
    "    'body_entropy', 'body_entropy_per_char', 'percent_digits'\n",
    "]\n",
    "\n",
    "results_lstm = {}\n",
    "\n",
    "for source_name in df['source'].unique():\n",
    "    print(f\"\\nğŸ”„ KERAS LOSO: Validating on source: {source_name}\")\n",
    "    \n",
    "    train = df[df['source'] != source_name]\n",
    "    test = df[df['source'] == source_name]\n",
    "\n",
    "    # Tokenization\n",
    "    tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, oov_token=\"<OOV>\")\n",
    "    tokenizer.fit_on_texts(train[text_col])\n",
    "    \n",
    "    X_train_text = pad_sequences(tokenizer.texts_to_sequences(train[text_col]), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    X_test_text = pad_sequences(tokenizer.texts_to_sequences(test[text_col]), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "    # Numeric preprocessing\n",
    "    scaler = StandardScaler()\n",
    "    X_train_num = scaler.fit_transform(train[num_cols])\n",
    "    X_test_num = scaler.transform(test[num_cols])\n",
    "    \n",
    "    y_train = train['label'].values\n",
    "    y_test = test['label'].values\n",
    "\n",
    "    # Model\n",
    "    input_text = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n",
    "    x_text = Embedding(MAX_NUM_WORDS, 128)(input_text)\n",
    "    x_text = LSTM(64)(x_text)\n",
    "\n",
    "    input_num = Input(shape=(X_train_num.shape[1],), name='num_input')\n",
    "    x_num = Dense(32, activation='relu')(input_num)\n",
    "\n",
    "    x = Concatenate()([x_text, x_num])\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=[input_text, input_num], outputs=output)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model.fit(\n",
    "        [X_train_text, X_train_num], y_train,\n",
    "        epochs=5, batch_size=32,\n",
    "        verbose=0, validation_split=0.1\n",
    "    )\n",
    "\n",
    "    y_pred = (model.predict([X_test_text, X_test_num]) > 0.5).astype(int).flatten()\n",
    "    report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "\n",
    "    results_lstm[source_name] = {\n",
    "        'support': len(y_test),\n",
    "        'accuracy': (y_pred == y_test).mean(),\n",
    "        'precision': report['1']['precision'],\n",
    "        'recall': report['1']['recall'],\n",
    "        'f1-score': report['1']['f1-score']\n",
    "    }\n",
    "\n",
    "df_results_lstm = pd.DataFrame(results_lstm).T\n",
    "print(\"\\nğŸ“Š KERAS LSTM LOSO Results:\\n\", df_results_lstm.round(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aafa9e3",
   "metadata": {},
   "source": [
    "### Migliorato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc4d198e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”„ KERAS LOSO: Validating on source: Assassin\n",
      "\u001b[1m182/182\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 96ms/step\n",
      "\n",
      "ğŸ”„ KERAS LOSO: Validating on source: CEAS-08\n",
      "\u001b[1m1224/1224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 59ms/step\n",
      "\n",
      "ğŸ”„ KERAS LOSO: Validating on source: Nigerian_Fraud\n",
      "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 58ms/step\n",
      "\n",
      "ğŸ”„ KERAS LOSO: Validating on source: Enron\n",
      "\u001b[1m931/931\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 60ms/step\n",
      "\n",
      "ğŸ”„ KERAS LOSO: Validating on source: Ling\n",
      "\u001b[1m90/90\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 56ms/step\n",
      "\n",
      "ğŸ“Š KERAS LSTM LOSO Results:\n",
      "                 support  accuracy  precision  recall  f1-score\n",
      "Assassin         5809.0     0.880      0.767   0.855     0.808\n",
      "CEAS-08         39154.0     0.834      0.852   0.850     0.851\n",
      "Nigerian_Fraud   3332.0     0.917      1.000   0.917     0.957\n",
      "Enron           29767.0     0.783      0.765   0.775     0.770\n",
      "Ling             2859.0     0.876      0.568   0.954     0.712\n"
     ]
    }
   ],
   "source": [
    "set_random_seed(42)     # per riproducibilitÃ \n",
    "\n",
    "# Settings\n",
    "MAX_NUM_WORDS = 10000\n",
    "MAX_SEQUENCE_LENGTH = 300\n",
    "text_col = 'text'\n",
    "num_cols = [\n",
    "    'subject_len', 'body_len', 'subject_density', 'body_density',\n",
    "    'num_exclamations', 'percent_punct',\n",
    "    'body_entropy', 'body_entropy_per_char', 'percent_digits'\n",
    "]\n",
    "\n",
    "results_lstm = {}\n",
    "\n",
    "for source_name in df['source'].unique():\n",
    "    print(f\"\\nğŸ”„ KERAS LOSO: Validating on source: {source_name}\")\n",
    "    \n",
    "    train = df[df['source'] != source_name]\n",
    "    test = df[df['source'] == source_name]\n",
    "\n",
    "    # Tokenization\n",
    "    tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, oov_token=\"<OOV>\")\n",
    "    tokenizer.fit_on_texts(train[text_col])\n",
    "    \n",
    "    X_train_text = pad_sequences(tokenizer.texts_to_sequences(train[text_col]), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    X_test_text = pad_sequences(tokenizer.texts_to_sequences(test[text_col]), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "    # Numeric preprocessing\n",
    "    scaler = StandardScaler()\n",
    "    X_train_num = scaler.fit_transform(train[num_cols])\n",
    "    X_test_num = scaler.transform(test[num_cols])\n",
    "    \n",
    "    y_train = train['label'].values\n",
    "    y_test = test['label'].values\n",
    "\n",
    "    # Model\n",
    "    input_text = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n",
    "    x_text = Embedding(MAX_NUM_WORDS, 128)(input_text)\n",
    "    # x_text = LSTM(64)(x_text)\n",
    "    x_text = Bidirectional(LSTM(64))(x_text)\n",
    "\n",
    "    input_num = Input(shape=(X_train_num.shape[1],), name='num_input')\n",
    "    x_num = Dense(32, activation='relu')(input_num)\n",
    "\n",
    "    x = Concatenate()([x_text, x_num])\n",
    "    x = Dense(64, activation='relu', kernel_regularizer=L2(0.001))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "    class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "    model = Model(inputs=[input_text, input_num], outputs=output)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "\n",
    "    model.fit(\n",
    "        [X_train_text, X_train_num], y_train,\n",
    "        epochs=5, batch_size=32, verbose=0, validation_split=0.1,\n",
    "        class_weight=class_weights, callbacks=[early_stop]\n",
    "    )\n",
    "\n",
    "    y_pred = (model.predict([X_test_text, X_test_num]) > 0.5).astype(int).flatten()\n",
    "    report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "\n",
    "    results_lstm[source_name] = {\n",
    "        'support': len(y_test),\n",
    "        'accuracy': (y_pred == y_test).mean(),\n",
    "        'precision': report['1']['precision'],\n",
    "        'recall': report['1']['recall'],\n",
    "        'f1-score': report['1']['f1-score']\n",
    "    }\n",
    "\n",
    "df_results_lstm = pd.DataFrame(results_lstm).T\n",
    "print(\"\\nğŸ“Š KERAS LSTM LOSO Results:\\n\", df_results_lstm.round(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26f46ca",
   "metadata": {},
   "source": [
    "### LSTM x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffd5b318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”„ KERAS LOSO: Validating on source: Assassin\n",
      "\n",
      "ğŸ”„ KERAS LOSO: Validating on source: CEAS-08\n",
      "\n",
      "ğŸ”„ KERAS LOSO: Validating on source: Nigerian_Fraud\n",
      "\n",
      "ğŸ”„ KERAS LOSO: Validating on source: Enron\n",
      "\n",
      "ğŸ”„ KERAS LOSO: Validating on source: Ling\n",
      "\n",
      "ğŸ“Š KERAS LSTM LOSO Results:\n",
      "                 support  accuracy  precision  recall  f1-score\n",
      "Assassin         5809.0     0.891      0.777   0.885     0.828\n",
      "CEAS-08         39154.0     0.836      0.913   0.782     0.842\n",
      "Nigerian_Fraud   3332.0     0.871      1.000   0.871     0.931\n",
      "Enron           29767.0     0.794      0.765   0.809     0.786\n",
      "Ling             2859.0     0.883      0.583   0.954     0.724\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Bidirectional, Dense, Dropout, Concatenate\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "set_random_seed(42)\n",
    "\n",
    "# Settings\n",
    "MAX_NUM_WORDS = 10000\n",
    "MAX_SEQUENCE_LENGTH = 300\n",
    "EMBED_DIM = 128  # puoi passare a 100 se usi GloVe 100d\n",
    "text_col = 'text'\n",
    "num_cols = [\n",
    "    'subject_len', 'body_len', 'subject_density', 'body_density',\n",
    "    'num_exclamations', 'percent_punct',\n",
    "    'body_entropy', 'body_entropy_per_char', 'percent_digits'\n",
    "]\n",
    "\n",
    "results_lstm = {}\n",
    "\n",
    "for source_name in df['source'].unique():\n",
    "    print(f\"\\nğŸ”„ KERAS LOSO: Validating on source: {source_name}\")\n",
    "    \n",
    "    train = df[df['source'] != source_name]\n",
    "    test = df[df['source'] == source_name]\n",
    "\n",
    "    # Tokenization\n",
    "    tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, oov_token=\"<OOV>\")\n",
    "    tokenizer.fit_on_texts(train[text_col])\n",
    "    \n",
    "    X_train_text = pad_sequences(tokenizer.texts_to_sequences(train[text_col]), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    X_test_text = pad_sequences(tokenizer.texts_to_sequences(test[text_col]), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "    # Numeric preprocessing\n",
    "    scaler = StandardScaler()\n",
    "    X_train_num = scaler.fit_transform(train[num_cols])\n",
    "    X_test_num = scaler.transform(test[num_cols])\n",
    "    \n",
    "    y_train = train['label'].values\n",
    "    y_test = test['label'].values\n",
    "\n",
    "    # Class balancing\n",
    "    class_weights = class_weight.compute_class_weight(class_weight='balanced',\n",
    "                                                      classes=np.unique(y_train),\n",
    "                                                      y=y_train)\n",
    "    class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "    # TEXT INPUT\n",
    "    input_text = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n",
    "    \n",
    "    # Optional: use pre-trained GloVe embeddings here\n",
    "    x_text = Embedding(MAX_NUM_WORDS, EMBED_DIM)(input_text)\n",
    "    x_text = Bidirectional(LSTM(64))(x_text)  # Bidirectional for better context\n",
    "\n",
    "    # NUMERIC INPUT\n",
    "    input_num = Input(shape=(X_train_num.shape[1],), name='num_input')\n",
    "    x_num = Dense(32, activation='relu', kernel_regularizer=l2(0.001))(input_num)\n",
    "\n",
    "    # MERGE\n",
    "    x = Concatenate()([x_text, x_num])\n",
    "    x = Dense(64, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "    x = Dropout(0.5)(x)  # more dropout for regularization\n",
    "    output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=[input_text, input_num], outputs=output)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Early stopping\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "\n",
    "    model.fit(\n",
    "        [X_train_text, X_train_num], y_train,\n",
    "        epochs=20,\n",
    "        batch_size=32,\n",
    "        validation_split=0.1,\n",
    "        verbose=0,\n",
    "        class_weight=class_weights,\n",
    "        callbacks=[early_stop]\n",
    "    )\n",
    "\n",
    "    y_pred = (model.predict([X_test_text, X_test_num], verbose=0) > 0.5).astype(int).flatten()\n",
    "    report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "\n",
    "    results_lstm[source_name] = {\n",
    "        'support': len(y_test),\n",
    "        'accuracy': (y_pred == y_test).mean(),\n",
    "        'precision': report['1']['precision'],\n",
    "        'recall': report['1']['recall'],\n",
    "        'f1-score': report['1']['f1-score']\n",
    "    }\n",
    "\n",
    "df_results_lstm = pd.DataFrame(results_lstm).T\n",
    "print(\"\\nğŸ“Š KERAS LSTM LOSO Results:\\n\", df_results_lstm.round(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c645e9d",
   "metadata": {},
   "source": [
    "### LSTM senza feature numeriche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "455fec50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”„ KERAS LOSO (Text-only): Validating on source: Assassin\n",
      "\n",
      "ğŸ”„ KERAS LOSO (Text-only): Validating on source: CEAS-08\n",
      "\n",
      "ğŸ”„ KERAS LOSO (Text-only): Validating on source: Nigerian_Fraud\n",
      "\n",
      "ğŸ”„ KERAS LOSO (Text-only): Validating on source: Enron\n",
      "\n",
      "ğŸ”„ KERAS LOSO (Text-only): Validating on source: Ling\n",
      "\n",
      "ğŸ“Š KERAS LSTM (Text-only) LOSO Results:\n",
      "                 support  accuracy  precision  recall  f1-score\n",
      "Assassin         5809.0     0.747      0.543   0.913     0.681\n",
      "CEAS-08         39154.0     0.802      0.835   0.805     0.819\n",
      "Nigerian_Fraud   3332.0     0.941      1.000   0.941     0.970\n",
      "Enron           29767.0     0.797      0.789   0.776     0.782\n",
      "Ling             2859.0     0.902      0.633   0.924     0.751\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Bidirectional, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "set_random_seed(42)\n",
    "\n",
    "# Settings\n",
    "MAX_NUM_WORDS = 10000\n",
    "MAX_SEQUENCE_LENGTH = 300\n",
    "EMBED_DIM = 128\n",
    "text_col = 'text'\n",
    "\n",
    "results_lstm_text_only = {}\n",
    "\n",
    "for source_name in df['source'].unique():\n",
    "    print(f\"\\nğŸ”„ KERAS LOSO (Text-only): Validating on source: {source_name}\")\n",
    "    \n",
    "    train = df[df['source'] != source_name]\n",
    "    test = df[df['source'] == source_name]\n",
    "\n",
    "    # Tokenization\n",
    "    tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, oov_token=\"<OOV>\")\n",
    "    tokenizer.fit_on_texts(train[text_col])\n",
    "    \n",
    "    X_train_text = pad_sequences(tokenizer.texts_to_sequences(train[text_col]), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    X_test_text = pad_sequences(tokenizer.texts_to_sequences(test[text_col]), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "    y_train = train['label'].values\n",
    "    y_test = test['label'].values\n",
    "\n",
    "    # Class balancing\n",
    "    class_weights = class_weight.compute_class_weight(class_weight='balanced',\n",
    "                                                      classes=np.unique(y_train),\n",
    "                                                      y=y_train)\n",
    "    class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "    # TEXT MODEL\n",
    "    input_text = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n",
    "    x = Embedding(MAX_NUM_WORDS, EMBED_DIM)(input_text)\n",
    "    x = Bidirectional(LSTM(64))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=input_text, outputs=output)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "\n",
    "    model.fit(\n",
    "        X_train_text, y_train,\n",
    "        epochs=20,\n",
    "        batch_size=32,\n",
    "        validation_split=0.1,\n",
    "        verbose=0,\n",
    "        class_weight=class_weights,\n",
    "        callbacks=[early_stop]\n",
    "    )\n",
    "\n",
    "    y_pred = (model.predict(X_test_text, verbose=0) > 0.5).astype(int).flatten()\n",
    "    report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "\n",
    "    results_lstm_text_only[source_name] = {\n",
    "        'support': len(y_test),\n",
    "        'accuracy': (y_pred == y_test).mean(),\n",
    "        'precision': report['1']['precision'],\n",
    "        'recall': report['1']['recall'],\n",
    "        'f1-score': report['1']['f1-score']\n",
    "    }\n",
    "\n",
    "df_results_lstm_text_only = pd.DataFrame(results_lstm_text_only).T\n",
    "print(\"\\nğŸ“Š KERAS LSTM (Text-only) LOSO Results:\\n\", df_results_lstm_text_only.round(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b00802",
   "metadata": {},
   "source": [
    "# Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7de0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm # For a nice progress bar\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78ea9b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”„ BERT LOSO: Validating on source: Assassin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding BERT:  19%|â–ˆâ–Š        | 894/4793 [1:16:45<5:34:46,  5.15s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     30\u001b[39m test = df[df[\u001b[33m'\u001b[39m\u001b[33msource\u001b[39m\u001b[33m'\u001b[39m] == source_name]\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Encode BERT\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m X_train_bert = \u001b[43mencode_bert_batched\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext_col\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m X_test_bert = encode_bert_batched(test[text_col])\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# Numeric\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mencode_bert_batched\u001b[39m\u001b[34m(texts, batch_size)\u001b[39m\n\u001b[32m     16\u001b[39m tokens = {k: v.to(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m tokens.items()}\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     outputs = \u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Take the [CLS] token embedding for sentence representation\u001b[39;00m\n\u001b[32m     20\u001b[39m batch_embeddings = outputs.last_hidden_state[:, \u001b[32m0\u001b[39m, :].cpu().numpy()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gabriele.greco\\OneDrive - AGM Solutions\\Documenti\\GitHub\\phishing_email_classifier\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gabriele.greco\\OneDrive - AGM Solutions\\Documenti\\GitHub\\phishing_email_classifier\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gabriele.greco\\OneDrive - AGM Solutions\\Documenti\\GitHub\\phishing_email_classifier\\.venv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:996\u001b[39m, in \u001b[36mBertModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    989\u001b[39m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[32m    990\u001b[39m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[32m    991\u001b[39m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[32m    992\u001b[39m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[32m    993\u001b[39m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[32m    994\u001b[39m head_mask = \u001b[38;5;28mself\u001b[39m.get_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers)\n\u001b[32m--> \u001b[39m\u001b[32m996\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    997\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    998\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    999\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1000\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1001\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1002\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1005\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1006\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1008\u001b[39m sequence_output = encoder_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1009\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.pooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gabriele.greco\\OneDrive - AGM Solutions\\Documenti\\GitHub\\phishing_email_classifier\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gabriele.greco\\OneDrive - AGM Solutions\\Documenti\\GitHub\\phishing_email_classifier\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gabriele.greco\\OneDrive - AGM Solutions\\Documenti\\GitHub\\phishing_email_classifier\\.venv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:651\u001b[39m, in \u001b[36mBertEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    648\u001b[39m layer_head_mask = head_mask[i] \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    649\u001b[39m past_key_value = past_key_values[i] \u001b[38;5;28;01mif\u001b[39;00m past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m651\u001b[39m layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# as a positional argument for gradient checkpointing\u001b[39;49;00m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    659\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    661\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    662\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gabriele.greco\\OneDrive - AGM Solutions\\Documenti\\GitHub\\phishing_email_classifier\\.venv\\Lib\\site-packages\\transformers\\modeling_layers.py:83\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     80\u001b[39m         logger.warning(message)\n\u001b[32m     82\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gabriele.greco\\OneDrive - AGM Solutions\\Documenti\\GitHub\\phishing_email_classifier\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gabriele.greco\\OneDrive - AGM Solutions\\Documenti\\GitHub\\phishing_email_classifier\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gabriele.greco\\OneDrive - AGM Solutions\\Documenti\\GitHub\\phishing_email_classifier\\.venv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:553\u001b[39m, in \u001b[36mBertLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[39m\n\u001b[32m    541\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    542\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    543\u001b[39m     hidden_states: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    550\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[torch.Tensor]:\n\u001b[32m    551\u001b[39m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[32m    552\u001b[39m     self_attn_past_key_value = past_key_value[:\u001b[32m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m553\u001b[39m     self_attention_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    554\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    555\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    556\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    558\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    559\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    560\u001b[39m     attention_output = self_attention_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    562\u001b[39m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gabriele.greco\\OneDrive - AGM Solutions\\Documenti\\GitHub\\phishing_email_classifier\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gabriele.greco\\OneDrive - AGM Solutions\\Documenti\\GitHub\\phishing_email_classifier\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gabriele.greco\\OneDrive - AGM Solutions\\Documenti\\GitHub\\phishing_email_classifier\\.venv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:483\u001b[39m, in \u001b[36mBertAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[39m\n\u001b[32m    473\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    474\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    475\u001b[39m     hidden_states: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    481\u001b[39m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    482\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[torch.Tensor]:\n\u001b[32m--> \u001b[39m\u001b[32m483\u001b[39m     self_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    492\u001b[39m     attention_output = \u001b[38;5;28mself\u001b[39m.output(self_outputs[\u001b[32m0\u001b[39m], hidden_states)\n\u001b[32m    493\u001b[39m     outputs = (attention_output,) + self_outputs[\u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gabriele.greco\\OneDrive - AGM Solutions\\Documenti\\GitHub\\phishing_email_classifier\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gabriele.greco\\OneDrive - AGM Solutions\\Documenti\\GitHub\\phishing_email_classifier\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gabriele.greco\\OneDrive - AGM Solutions\\Documenti\\GitHub\\phishing_email_classifier\\.venv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:376\u001b[39m, in \u001b[36mBertSdpaSelfAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[39m\n\u001b[32m    374\u001b[39m     key_layer, value_layer = past_key_value\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m376\u001b[39m     key_layer = \u001b[38;5;28mself\u001b[39m.transpose_for_scores(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_states\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    377\u001b[39m     value_layer = \u001b[38;5;28mself\u001b[39m.transpose_for_scores(\u001b[38;5;28mself\u001b[39m.value(current_states))\n\u001b[32m    378\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cross_attention:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gabriele.greco\\OneDrive - AGM Solutions\\Documenti\\GitHub\\phishing_email_classifier\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gabriele.greco\\OneDrive - AGM Solutions\\Documenti\\GitHub\\phishing_email_classifier\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gabriele.greco\\OneDrive - AGM Solutions\\Documenti\\GitHub\\phishing_email_classifier\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "bert = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert.eval()\n",
    "\n",
    "def encode_bert_batched(texts, batch_size=16): # Adjust batch_size as needed\n",
    "    all_embeddings = []\n",
    "    # Convert texts to a list if not already to enable easy slicing\n",
    "    texts_list = list(texts)\n",
    "    for i in tqdm(range(0, len(texts_list), batch_size), desc=\"Encoding BERT\"):\n",
    "        batch_texts = texts_list[i:i + batch_size]\n",
    "        tokens = tokenizer(\n",
    "            batch_texts, padding=True, truncation=True, max_length=256,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        tokens = {k: v.to(device) for k, v in tokens.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = bert(**tokens)\n",
    "        # Take the [CLS] token embedding for sentence representation\n",
    "        batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "        all_embeddings.append(batch_embeddings)\n",
    "    return np.vstack(all_embeddings)\n",
    "\n",
    "results_bert = {}\n",
    "\n",
    "for source_name in df['source'].unique():\n",
    "    print(f\"\\nğŸ”„ BERT LOSO: Validating on source: {source_name}\")\n",
    "\n",
    "    train = df[df['source'] != source_name]\n",
    "    test = df[df['source'] == source_name]\n",
    "\n",
    "    # Encode BERT\n",
    "    X_train_bert = encode_bert_batched(train[text_col])\n",
    "    X_test_bert = encode_bert_batched(test[text_col])\n",
    "    \n",
    "    # Numeric\n",
    "    scaler = StandardScaler()\n",
    "    X_train_num = scaler.fit_transform(train[num_cols])\n",
    "    X_test_num = scaler.transform(test[num_cols])\n",
    "\n",
    "    # Combine features\n",
    "    X_train = np.hstack([X_train_bert, X_train_num])\n",
    "    X_test = np.hstack([X_test_bert, X_test_num])\n",
    "\n",
    "    y_train = train['label'].values\n",
    "    y_test = test['label'].values\n",
    "\n",
    "    # Classifier\n",
    "    clf = LogisticRegression(max_iter=1000)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "\n",
    "    results_bert[source_name] = {\n",
    "        'support': len(y_test),\n",
    "        'accuracy': (y_pred == y_test).mean(),\n",
    "        'precision': report['1']['precision'],\n",
    "        'recall': report['1']['recall'],\n",
    "        'f1-score': report['1']['f1-score']\n",
    "    }\n",
    "\n",
    "df_results_bert = pd.DataFrame(results_bert).T\n",
    "print(\"\\nğŸ“Š BERT + Numeric LOSO Results:\\n\", df_results_bert.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e335ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Save BERT Model and Tokenizer\n",
    "# Hugging Face models have a convenient .save_pretrained() method\n",
    "# This saves the model weights and configuration.\n",
    "# The tokenizer saves its vocabulary and configuration.\n",
    "model_dir = \"./saved_phishing_model\"\n",
    "os.makedirs(model_dir, exist_ok=True) # Create directory if it doesn't exist\n",
    "\n",
    "bert.save_pretrained(model_dir)\n",
    "tokenizer.save_pretrained(model_dir)\n",
    "print(f\"BERT model and tokenizer saved to: {model_dir}\")\n",
    "\n",
    "# 2. Save StandardScaler\n",
    "# Use joblib for scikit-learn objects (scalers, classifiers)\n",
    "scaler_path = os.path.join(model_dir, \"scaler.joblib\")\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f\"Scaler saved to: {scaler_path}\")\n",
    "\n",
    "# 3. Save Logistic Regression Classifier\n",
    "clf_path = os.path.join(model_dir, \"logistic_regression_clf.joblib\")\n",
    "joblib.dump(clf, clf_path)\n",
    "print(f\"Logistic Regression classifier saved to: {clf_path}\")\n",
    "\n",
    "# 4. Save feature column names (important for consistent processing)\n",
    "feature_config_path = os.path.join(model_dir, \"feature_config.json\")\n",
    "import json\n",
    "feature_config = {\n",
    "    \"text_column\": text_col,\n",
    "    \"numeric_columns\": num_cols\n",
    "}\n",
    "with open(feature_config_path, 'w') as f:\n",
    "    json.dump(feature_config, f)\n",
    "print(f\"Feature configuration saved to: {feature_config_path}\")\n",
    "\n",
    "print(\"All necessary components saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4933d9",
   "metadata": {},
   "source": [
    "# BERT senza feature numeriche\n",
    "- prima opzione con solo il testo\n",
    "- fine-tune bert-base-uncased con BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9d8b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a8d7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "text_col = \"text\"\n",
    "label_col = \"label\"\n",
    "num_labels = 2\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "class EmailDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.encodings = tokenizer(\n",
    "            texts.tolist(), truncation=True, padding=True, max_length=256, return_tensors=\"pt\"\n",
    "        )\n",
    "        self.labels = torch.tensor(labels.tolist())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: v[idx] for k, v in self.encodings.items()}\n",
    "        item[\"labels\"] = self.labels[idx]\n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aea1e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_finetune = {}\n",
    "\n",
    "for source_name in df[\"source\"].unique():\n",
    "    print(f\"\\nğŸ” Fine-tuning BERT â€” Validating on source: {source_name}\")\n",
    "\n",
    "    train = df[df[\"source\"] != source_name]\n",
    "    test = df[df[\"source\"] == source_name]\n",
    "\n",
    "    train_dataset = EmailDataset(train[text_col], train[label_col])\n",
    "    test_dataset = EmailDataset(test[text_col], test[label_col])\n",
    "\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        \"bert-base-uncased\", num_labels=num_labels\n",
    "    )\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=16,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=10,\n",
    "        save_strategy=\"no\",\n",
    "        report_to=\"none\",\n",
    "        load_best_model_at_end=False,\n",
    "        disable_tqdm=True\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # Prediction\n",
    "    preds_output = trainer.predict(test_dataset)\n",
    "    preds = np.argmax(preds_output.predictions, axis=1)\n",
    "    y_true = test[label_col].values\n",
    "\n",
    "    report = classification_report(y_true, preds, output_dict=True, zero_division=0)\n",
    "    results_finetune[source_name] = {\n",
    "        'support': len(y_true),\n",
    "        'accuracy': (preds == y_true).mean(),\n",
    "        'precision': report['1']['precision'],\n",
    "        'recall': report['1']['recall'],\n",
    "        'f1-score': report['1']['f1-score']\n",
    "    }\n",
    "\n",
    "df_results_finetune = pd.DataFrame(results_finetune).T\n",
    "print(\"\\nğŸ“Š Fine-Tuned BERT LOSO Results:\\n\", df_results_finetune.round(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f8d607",
   "metadata": {},
   "source": [
    "# BERT con feature numeriche\n",
    "- Usare BERT (bert-base-uncased) per estrarre l'embedding dal testo (subject + body)\n",
    "- concatenare le feature numeriche\n",
    "- passare il vettore combinato a un MLP (Multilayer Perceptron) per la calssficiazione binaria\n",
    "- allenare tutto end-to-end, incluso il fine tuning di BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cd5e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0a6d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmailDatasetWithFeatures(Dataset):\n",
    "    def __init__(self, texts, num_feats, labels, tokenizer, max_len=256):\n",
    "        self.encodings = tokenizer(\n",
    "            texts.tolist(), truncation=True, padding=True, max_length=max_len, return_tensors=\"pt\"\n",
    "        )\n",
    "        self.num_feats = torch.tensor(num_feats, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels.tolist(), dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: v[idx] for k, v in self.encodings.items()}\n",
    "        item[\"num_feats\"] = self.num_feats[idx]\n",
    "        item[\"labels\"] = self.labels[idx]\n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa814b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertWithNumeric(nn.Module):\n",
    "    def __init__(self, num_numeric_features, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Combiner: CLS embedding (768) + numeric\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(768 + num_numeric_features, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, num_feats):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]  # [CLS]\n",
    "        combined = torch.cat((cls_output, num_feats), dim=1)\n",
    "        return self.classifier(combined).squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef13fb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "text_col = \"text\"\n",
    "label_col = \"label\"\n",
    "num_cols = [\n",
    "    'subject_len', 'body_len', 'subject_density', 'body_density',\n",
    "    'num_exclamations', 'percent_punct',\n",
    "    'body_entropy', 'body_entropy_per_char', 'percent_digits'\n",
    "]\n",
    "\n",
    "results_finetune = {}\n",
    "\n",
    "for source_name in df[\"source\"].unique():\n",
    "    print(f\"\\nğŸš€ Fine-tuning BERT+Numeric â€” Validating on: {source_name}\")\n",
    "\n",
    "    train = df[df[\"source\"] != source_name]\n",
    "    test = df[df[\"source\"] == source_name]\n",
    "\n",
    "    # Scale numeric features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_num = scaler.fit_transform(train[num_cols])\n",
    "    X_test_num = scaler.transform(test[num_cols])\n",
    "\n",
    "    y_train = train[label_col].values\n",
    "    y_test = test[label_col].values\n",
    "\n",
    "    # Datasets & Loaders\n",
    "    train_dataset = EmailDatasetWithFeatures(train[text_col], X_train_num, y_train, tokenizer)\n",
    "    test_dataset = EmailDatasetWithFeatures(test[text_col], X_test_num, y_test, tokenizer)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "    # Model\n",
    "    model = BertWithNumeric(num_numeric_features=X_train_num.shape[1]).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "    loss_fn = nn.BCELoss()\n",
    "\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(3):\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            num_feats = batch[\"num_feats\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask, num_feats)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            num_feats = batch[\"num_feats\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask, num_feats)\n",
    "            preds = (outputs > 0.5).long().cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "\n",
    "    report = classification_report(y_test, all_preds, output_dict=True, zero_division=0)\n",
    "\n",
    "    results_finetune[source_name] = {\n",
    "        'support': len(y_test),\n",
    "        'accuracy': np.mean(np.array(all_preds) == y_test),\n",
    "        'precision': report['1']['precision'],\n",
    "        'recall': report['1']['recall'],\n",
    "        'f1-score': report['1']['f1-score']\n",
    "    }\n",
    "\n",
    "df_results_finetune = pd.DataFrame(results_finetune).T\n",
    "print(\"\\nğŸ“Š Fine-tuned BERT + Numeric LOSO Results:\\n\", df_results_finetune.round(3))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
