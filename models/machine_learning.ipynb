{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b604a54",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fdcd38ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier  # o altri\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944870e1",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9b45f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../dataset/3_features_phishing_enhanced.csv\")\n",
    "print(df.shape)\n",
    "df = df.drop(columns=['num_links', 'num_special_chars', 'has_bank_word'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e0930e",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9204e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ff32f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98      7919\n",
      "           1       0.98      0.98      0.98      8579\n",
      "\n",
      "    accuracy                           0.98     16498\n",
      "   macro avg       0.98      0.98      0.98     16498\n",
      "weighted avg       0.98      0.98      0.98     16498\n",
      "\n",
      "[[7789  130]\n",
      " [ 134 8445]]\n"
     ]
    }
   ],
   "source": [
    "# text_col = ['text']  # body, subject, text (subject + body)\n",
    "num_cols = [\n",
    "    'subject_len', 'body_len', 'subject_density', 'body_density','num_links', 'num_special_chars',\n",
    "    'num_exclamations', 'body_entropy', 'body_entropy_per_char', 'percent_digits','has_bank_word', 'percent_punct'\n",
    "]\n",
    "\n",
    "text_col = 'text'\n",
    "\n",
    "# Split\n",
    "X = df[[text_col]+ num_cols]\n",
    "y = df['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocessing: text + numeriche\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('tfidf', TfidfVectorizer(max_features=5000, ngram_range=(1, 2)), text_col),\n",
    "    ('num', StandardScaler(), num_cols)\n",
    "])\n",
    "\n",
    "# Modello di partenza (cambia qui con altri modelli)\n",
    "pipeline = Pipeline([\n",
    "    ('preprocess', preprocessor),\n",
    "    ('clf', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Addestramento\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Valutazione\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "# ottengo valori alti, il modello generalizza troppo forse.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7aaf991",
   "metadata": {},
   "source": [
    "### Leave-one-out Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa74853a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Valutazione su sorgente esclusa: Assassin\n",
      "\n",
      "üîç Valutazione su sorgente esclusa: CEAS-08\n",
      "\n",
      "üîç Valutazione su sorgente esclusa: Nigerian_Fraud\n",
      "\n",
      "üîç Valutazione su sorgente esclusa: Nazario\n",
      "\n",
      "üîç Valutazione su sorgente esclusa: Enron\n",
      "\n",
      "üîç Valutazione su sorgente esclusa: Ling\n",
      "\n",
      "üìä Risultati Leave-One-Source-Out:\n",
      "\n",
      "                support  accuracy  precision  recall  f1-score\n",
      "Assassin         5809.0     0.878      0.801   0.782     0.792\n",
      "CEAS-08         39154.0     0.851      0.886   0.841     0.863\n",
      "Nigerian_Fraud   3332.0     0.957      1.000   0.957     0.978\n",
      "Nazario          1565.0     0.668      1.000   0.668     0.801\n",
      "Enron           29767.0     0.781      0.744   0.816     0.778\n",
      "Ling             2859.0     0.942      0.798   0.856     0.826\n"
     ]
    }
   ],
   "source": [
    "text_col = 'text'\n",
    "num_cols = [\n",
    "    'subject_len', 'body_len', 'subject_density', 'body_density',\n",
    "    'num_links', 'num_special_chars', 'num_exclamations',\n",
    "    'body_entropy', 'body_entropy_per_char', 'percent_digits',\n",
    "    'has_bank_word', 'percent_punct'\n",
    "]\n",
    "\n",
    "# Output complessivo\n",
    "results = {}\n",
    "\n",
    "# Loop Leave-One-Source-Out\n",
    "for source_name in df['source'].unique():\n",
    "    print(f\"\\nüîç Valutazione su sorgente esclusa: {source_name}\")\n",
    "    \n",
    "    train = df[df['source'] != source_name]\n",
    "    test = df[df['source'] == source_name]\n",
    "\n",
    "    # Features e label\n",
    "    X_train = train[[text_col] + num_cols]\n",
    "    y_train = train['label']\n",
    "    X_test = test[[text_col] + num_cols]\n",
    "    y_test = test['label']\n",
    "\n",
    "    # Pipeline\n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('tfidf', TfidfVectorizer(max_features=5000, ngram_range=(1, 2)), text_col),\n",
    "        ('num', StandardScaler(), num_cols)\n",
    "    ])\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocess', preprocessor),\n",
    "        ('clf', RandomForestClassifier(random_state=42))\n",
    "    ])\n",
    "\n",
    "    # Train\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Test\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "    \n",
    "    results[source_name] = {\n",
    "        'support': len(y_test),\n",
    "        'accuracy': (y_pred == y_test).mean(),\n",
    "        'precision': report['1']['precision'],\n",
    "        'recall': report['1']['recall'],\n",
    "        'f1-score': report['1']['f1-score']\n",
    "    }\n",
    "\n",
    "# Output riepilogativo\n",
    "df_results = pd.DataFrame(results).T\n",
    "print(\"\\nüìä Risultati Leave-One-Source-Out:\\n\")\n",
    "print(df_results.round(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8e7fd540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvataggio su file\n",
    "import pickle \n",
    "\n",
    "with open(\"dati.pkl\", \"wb\") as file:  # \"wb\" significa scrittura in modalit√† binaria\n",
    "    pickle.dump(df_results, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8008edf1",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74623abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Validazione su: Assassin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Validazione su: CEAS-08\n",
      "\n",
      "üîç Validazione su: Nigerian_Fraud\n",
      "\n",
      "üîç Validazione su: Nazario\n",
      "\n",
      "üîç Validazione su: Enron\n",
      "\n",
      "üîç Validazione su: Ling\n",
      "\n",
      "üìä Risultati LOSO:\n",
      "                 support  accuracy  precision  recall  f1-score\n",
      "Assassin         5809.0     0.881      0.789   0.814     0.801\n",
      "CEAS-08         39154.0     0.844      0.874   0.843     0.858\n",
      "Nigerian_Fraud   3332.0     0.971      1.000   0.971     0.985\n",
      "Nazario          1565.0     0.619      1.000   0.619     0.765\n",
      "Enron           29767.0     0.810      0.753   0.885     0.814\n",
      "Ling             2859.0     0.929      0.710   0.937     0.808\n"
     ]
    }
   ],
   "source": [
    "text_col = 'text'\n",
    "num_cols = [\n",
    "    'subject_len', 'body_len', 'subject_density', 'body_density',\n",
    "    'num_exclamations', 'percent_punct',\n",
    "    'body_entropy', 'body_entropy_per_char', 'percent_digits',\n",
    "    #'has_bank_word', 'num_links',  'num_special_chars'\n",
    "]\n",
    "\n",
    "# Preprocessing da riutilizzare\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('tfidf', TfidfVectorizer(max_features=10000, ngram_range=(1, 3),  min_df=5, max_df=0.8), text_col),\n",
    "    ('num', StandardScaler(), num_cols)\n",
    "])\n",
    "\n",
    "results = {}\n",
    "\n",
    "# Leave-One-Source-Out Evaluation\n",
    "for source_name in df['source'].unique():\n",
    "    print(f\"\\nüîç Validazione su: {source_name}\")\n",
    "    \n",
    "    train = df[df['source'] != source_name]\n",
    "    test = df[df['source'] == source_name]\n",
    "\n",
    "    X_train = train[[text_col] + num_cols]\n",
    "    y_train = train['label']\n",
    "    X_test = test[[text_col] + num_cols]\n",
    "    y_test = test['label']\n",
    "    neg, pos = np.bincount(y_train)\n",
    "    scale = neg / pos\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocess', preprocessor),\n",
    "        ('clf', XGBClassifier(eval_metric='logloss', scale_pos_weight=scale))\n",
    "    ])\n",
    "\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "    report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "\n",
    "    results[source_name] = {\n",
    "        'support': len(y_test),\n",
    "        'accuracy': (y_pred == y_test).mean(),\n",
    "        'precision': report['1']['precision'],\n",
    "        'recall': report['1']['recall'],\n",
    "        'f1-score': report['1']['f1-score']\n",
    "    }\n",
    "\n",
    "df_results = pd.DataFrame(results).T\n",
    "print(\"\\nüìä Risultati LOSO:\\n\", df_results.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08655212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final training on full dataset\n",
    "X_final = df[[text_col] + num_cols]\n",
    "y_final = df['label']\n",
    "\n",
    "final_pipeline = Pipeline([\n",
    "    ('preprocess', preprocessor),\n",
    "    ('clf', XGBClassifier(use_label_encoder=False, eval_metric='logloss'))\n",
    "])\n",
    "\n",
    "final_pipeline.fit(X_final, y_final)\n",
    "\n",
    "joblib.dump(final_pipeline, 'phishing_model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6a751f",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "import joblib\n",
    "import pandas as pd\n",
    "from fastapi import FastAPI, Request\n",
    "\n",
    "model = joblib.load('phishing_model.pkl')\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "async def predict_email(email: dict):\n",
    "    # Prepara il DataFrame con le stesse feature\n",
    "    df = pd.DataFrame([email])\n",
    "\n",
    "    # Calcola le feature numeriche custom (devi includere il codice che le estrae!)\n",
    "    # Esempio placeholder:\n",
    "    df['subject_len'] = df['subject'].apply(len)\n",
    "    df['body_len'] = df['body'].apply(len)\n",
    "    # ... calcola tutte le feature numeriche\n",
    "\n",
    "    df['text'] = df['subject'] + \" \" + df['body']\n",
    "\n",
    "    X = df[['text'] + num_cols]\n",
    "    prediction = model.predict(X)[0]\n",
    "\n",
    "    return {\"phishing\": bool(prediction)}\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30f8049",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779a7e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "# Colonne\n",
    "text_col = 'text'\n",
    "num_cols = [\n",
    "    'subject_len', 'body_len', 'subject_density', 'body_density',\n",
    "    'num_links', 'num_special_chars', 'num_exclamations',\n",
    "    'body_entropy', 'body_entropy_per_char', 'percent_digits',\n",
    "    'has_bank_word', 'percent_punct'\n",
    "]\n",
    "\n",
    "# Output complessivo\n",
    "results = {}\n",
    "\n",
    "# Loop Leave-One-Source-Out\n",
    "for source_name in df['source'].unique():\n",
    "    print(f\"\\nüîç Valutazione su sorgente esclusa: {source_name}\")\n",
    "    \n",
    "    train = df[df['source'] != source_name]\n",
    "    test = df[df['source'] == source_name]\n",
    "\n",
    "    # Features e label\n",
    "    X_train = train[[text_col] + num_cols]\n",
    "    y_train = train['label']\n",
    "    X_test = test[[text_col] + num_cols]\n",
    "    y_test = test['label']\n",
    "\n",
    "    # Pipeline\n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('tfidf', TfidfVectorizer(max_features=5000, ngram_range=(1, 2)), text_col),\n",
    "        ('num', StandardScaler(), num_cols)\n",
    "    ])\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocess', preprocessor),\n",
    "        ('clf', RandomForestClassifier(random_state=42))\n",
    "    ])\n",
    "\n",
    "    # Train\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Test\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "    \n",
    "    results[source_name] = {\n",
    "        'support': len(y_test),\n",
    "        'accuracy': (y_pred == y_test).mean(),\n",
    "        'precision': report['1']['precision'],\n",
    "        'recall': report['1']['recall'],\n",
    "        'f1-score': report['1']['f1-score']\n",
    "    }\n",
    "\n",
    "# Output riepilogativo\n",
    "df_results = pd.DataFrame(results).T\n",
    "print(\"\\nüìä Risultati Leave-One-Source-Out:\\n\")\n",
    "print(df_results.round(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4915ae8",
   "metadata": {},
   "source": [
    "## ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0415a55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Validazione su: Assassin\n",
      "\n",
      "üîç Validazione su: CEAS-08\n",
      "\n",
      "üîç Validazione su: Nigerian_Fraud\n",
      "\n",
      "üîç Validazione su: Nazario\n",
      "\n",
      "üîç Validazione su: Enron\n",
      "\n",
      "üîç Validazione su: Ling\n",
      "\n",
      "üìä Risultati LOSO:\n",
      "                 support  accuracy  precision  recall  f1-score\n",
      "Assassin         5809.0     0.764      0.562   0.919     0.697\n",
      "CEAS-08         39154.0     0.646      0.837   0.454     0.589\n",
      "Nigerian_Fraud   3332.0     0.522      1.000   0.522     0.686\n",
      "Nazario          1565.0     0.788      1.000   0.788     0.881\n",
      "Enron           29767.0     0.609      0.553   0.877     0.678\n",
      "Ling             2859.0     0.765      0.395   0.878     0.544\n"
     ]
    }
   ],
   "source": [
    "text_col = 'text'\n",
    "num_cols = [\n",
    "    'subject_len', 'body_len', 'subject_density', 'body_density',\n",
    "    'num_links', 'num_special_chars', 'num_exclamations',\n",
    "    'body_entropy', 'body_entropy_per_char', 'percent_digits',\n",
    "    'has_bank_word', 'percent_punct'\n",
    "]\n",
    "\n",
    "# Preprocessing da riutilizzare\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('tfidf', TfidfVectorizer(max_features=5000, ngram_range=(1, 2), min_df=5, max_df=0.8), text_col),\n",
    "    ('num', StandardScaler(), num_cols)\n",
    "])\n",
    "\n",
    "results = {}\n",
    "\n",
    "# Leave-One-Source-Out Evaluation\n",
    "for source_name in df['source'].unique():\n",
    "    print(f\"\\nüîç Validazione su: {source_name}\")\n",
    "    \n",
    "    train = df[df['source'] != source_name]\n",
    "    test = df[df['source'] == source_name]\n",
    "\n",
    "    X_train = train[[text_col] + num_cols]\n",
    "    y_train = train['label']\n",
    "    X_test = test[[text_col] + num_cols]\n",
    "    y_test = test['label']\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocess', preprocessor),\n",
    "        ('clf', ExtraTreesClassifier(n_estimators=100, max_depth=10, random_state=42))\n",
    "    ])\n",
    "\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "    report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "\n",
    "    results[source_name] = {\n",
    "        'support': len(y_test),\n",
    "        'accuracy': (y_pred == y_test).mean(),\n",
    "        'precision': report['1']['precision'],\n",
    "        'recall': report['1']['recall'],\n",
    "        'f1-score': report['1']['f1-score']\n",
    "    }\n",
    "\n",
    "df_results = pd.DataFrame(results).T\n",
    "print(\"\\nüìä Risultati LOSO:\\n\", df_results.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847d1f14",
   "metadata": {},
   "source": [
    "## ADB - Ada Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f597d90d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Validazione su: Assassin\n",
      "\n",
      "üîç Validazione su: CEAS-08\n",
      "\n",
      "üîç Validazione su: Nigerian_Fraud\n",
      "\n",
      "üîç Validazione su: Nazario\n",
      "\n",
      "üîç Validazione su: Enron\n",
      "\n",
      "üîç Validazione su: Ling\n",
      "\n",
      "üìä Risultati LOSO:\n",
      "                 support  accuracy  precision  recall  f1-score\n",
      "Assassin         5809.0     0.777      0.594   0.776     0.673\n",
      "CEAS-08         39154.0     0.718      0.821   0.631     0.714\n",
      "Nigerian_Fraud   3332.0     0.779      1.000   0.779     0.876\n",
      "Nazario          1565.0     0.781      1.000   0.781     0.877\n",
      "Enron           29767.0     0.671      0.599   0.903     0.720\n",
      "Ling             2859.0     0.750      0.375   0.836     0.518\n"
     ]
    }
   ],
   "source": [
    "text_col = 'text'\n",
    "num_cols = [\n",
    "    'subject_len', 'body_len', 'subject_density', 'body_density',\n",
    "    'num_links', 'num_special_chars', 'num_exclamations',\n",
    "    'body_entropy', 'body_entropy_per_char', 'percent_digits',\n",
    "    'has_bank_word', 'percent_punct'\n",
    "]\n",
    "\n",
    "# Preprocessing da riutilizzare\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('tfidf', TfidfVectorizer(max_features=5000, ngram_range=(1, 2), min_df=5, max_df=0.8), text_col),\n",
    "    ('num', StandardScaler(), num_cols)\n",
    "])\n",
    "\n",
    "results = {}\n",
    "\n",
    "# Leave-One-Source-Out Evaluation\n",
    "for source_name in df['source'].unique():\n",
    "    print(f\"\\nüîç Validazione su: {source_name}\")\n",
    "    \n",
    "    train = df[df['source'] != source_name]\n",
    "    test = df[df['source'] == source_name]\n",
    "\n",
    "    X_train = train[[text_col] + num_cols]\n",
    "    y_train = train['label']\n",
    "    X_test = test[[text_col] + num_cols]\n",
    "    y_test = test['label']\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocess', preprocessor),\n",
    "        ('clf', AdaBoostClassifier(n_estimators=100, learning_rate=0.5))\n",
    "    ])\n",
    "\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "    report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "\n",
    "    results[source_name] = {\n",
    "        'support': len(y_test),\n",
    "        'accuracy': (y_pred == y_test).mean(),\n",
    "        'precision': report['1']['precision'],\n",
    "        'recall': report['1']['recall'],\n",
    "        'f1-score': report['1']['f1-score']\n",
    "    }\n",
    "\n",
    "df_results = pd.DataFrame(results).T\n",
    "print(\"\\nüìä Risultati LOSO:\\n\", df_results.round(3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
